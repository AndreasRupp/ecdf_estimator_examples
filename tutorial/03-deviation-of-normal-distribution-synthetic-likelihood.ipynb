{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AndreasRupp/ecdf_estimator_examples/blob/bsc_thesis/tutorial/03-deviation-of-normal-distribution-synthetic-likelihood.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH4RcKXcFdfg"
      },
      "source": [
        "# Parameter Identification for Normal Distribution Data - Synthetic Likelihood Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bQ91S0KFoS6"
      },
      "source": [
        "This notebook introduces a synthetic likelihood parameter identification method for identifying deviation parameters in simple normal distribution data. The approach is based on the empirical cumulative distribution function (eCDF).\n",
        "\n",
        "When eCDF's standard and bootsrap method are used normally, the objective function is created with the training data, and then new data with different parameters is evaluated with that same function. However, using standard or bootstrap method with synthetic likelihood this is other way around: the objective functions are created with the new datasets with different parameters, and the training data, deviation of which is identified, is then evaluated with these objective functions.\n",
        "\n",
        "The implementation of the method using Python and ecdf-estimator package is explained step-by-step within this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXuQ5sVMXmm"
      },
      "source": [
        "## Setup of the Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWejq00lMbsG"
      },
      "source": [
        "### Environment Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twXOUE0pNhvG"
      },
      "source": [
        "First, `ecdf-estimator` is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFNFJ1hprMZg",
        "outputId": "313a87db-df80-4779-84ae-c7f4835bfdbb"
      },
      "outputs": [],
      "source": [
        "pip install --ignore-requires-python ecdf-estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBwtVujVNmae"
      },
      "source": [
        "After the installation, the estimator is imported so that its' functions can actually be used. It's abbreviated as `ecdf` so it's easier and faster to write in the code and makes it easier to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pofbzaOZrjtm"
      },
      "outputs": [],
      "source": [
        "import ecdf_estimator as ecdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLZYAb1-PSL-"
      },
      "source": [
        "Next, the other two necessary modules, `numpy` and `matplotlib.pyplot`, are imported. The modules are used to do array operations efficiently and to visualize the data and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtbFiqCTrlr5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLrUsxiUPWe1"
      },
      "source": [
        "### Creation of Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEHOD3V4PsyU"
      },
      "source": [
        "Next, normally distributed training data must be created. First, the variables which are used to create the data are defined:\n",
        "\n",
        "* `mean`: The mean value of training data.<br>\n",
        "* `dev`: The training data's standard deviation, which is tried to identidy.<br>\n",
        "* `size`: The number of created data points.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOsRmP9lro0_"
      },
      "outputs": [],
      "source": [
        "mean = 0\n",
        "dev = 5\n",
        "size = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKGsirw6PxT3"
      },
      "source": [
        "After the parameters are defined, the normally distibuted training data can be created using them and `numpy`'s `random.normal` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "morTF1IqrtXu"
      },
      "outputs": [],
      "source": [
        "data = np.random.normal(mean, dev, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXtk6TIdP3Ic"
      },
      "source": [
        "## Parameter Identification Process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF4beJQcP7t-"
      },
      "source": [
        "After the creation of training data, the actual parameter identification begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbrBGOn1P_cK"
      },
      "source": [
        "### Definition of Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzS588KfQJNs"
      },
      "source": [
        "The variables which will be used in the process are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSgSYinoZVTB"
      },
      "source": [
        "* `ecdf_type`: Defines the identification method used. By default `standard` method is used, but also `bootstrap` is possible. Especially with bootstrapping, smaller sized training data could be used to keep computing efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZf6YoVfZbtu"
      },
      "outputs": [],
      "source": [
        "ecdf_type = \"standard\"\n",
        "#ecdf_type = \"bootstrap\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffXJmZwKQKiA"
      },
      "source": [
        "* `subset_sizes`: The list of subset sizes of new datasets which are created later. Note that the sum of subset sizes defines how many data values from the training data are used in the evaluation process. Basically, it only means that values from larger training data will be ignored. In bootstrapping, only the first two datasets define the sizes of the subsets which are chosen in the bootstrapping process. However, the length of the list defines the number of new datasets created for each parameter tried so it can be more than 2 also for bootstrapping to get more reliable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6zMRZfwsG11"
      },
      "outputs": [],
      "source": [
        "subset_sizes = [100]*10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgQP4fO2AIoQ"
      },
      "source": [
        "* `n_newsets`: A variable `n_newsets` defines how many new datasets are created for each deviation parameter and evaluated with the objective function as mentioned before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2tr0QmTAGPO"
      },
      "outputs": [],
      "source": [
        "n_newsets = len(subset_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UHKlRXvapxP"
      },
      "source": [
        "* `mean_new`: The mean value of new datasets to be created. In practice, this is now the same as the mean of the training data, but this variable is defined for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbtU77dWsKgl"
      },
      "outputs": [],
      "source": [
        "mean_new = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve0z70GWasdL"
      },
      "source": [
        "* `n_bins`: The maximum number of bins which will be used for the eCDF-vectors. The exact number will be defined by the `ecdf`'s own functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htUTMwcJsMk_"
      },
      "outputs": [],
      "source": [
        "n_bins = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxdwLVEhazVT"
      },
      "source": [
        "- `min_dev`: The start parameter to be estimated.<br>\n",
        "- `max_dev`: The end parameter to be estimated.<br>\n",
        "\n",
        "`min_dev` and `max_dev` define the interval between which all integer values are tested to be the standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdAif0RWsQJl"
      },
      "outputs": [],
      "source": [
        "min_dev = 3\n",
        "max_dev = 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxEcaTN3fNQY"
      },
      "source": [
        "- `devs`: A list which includes all the deviation parameters which will be evaluated, is created.\n",
        "- `n_devs`: The number of parameters to be tried is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vvXqgwssYur"
      },
      "outputs": [],
      "source": [
        "devs = list(range(min_dev, max_dev + 1))\n",
        "n_devs = len(devs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT8n6PhIgaFE"
      },
      "source": [
        "Next, a function named distance, which has a major part in the creation of eCDF-vectors in the whole identification process, must be defined. The function computes the absolute value of the subtraction of the data points, which is the so called Euclidian distance between the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJ70P0HsTbB"
      },
      "outputs": [],
      "source": [
        "def distance(data_a, data_b):\n",
        "  return np.abs(data_a-data_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asw4obbFbCkP"
      },
      "source": [
        "### Evaluation of Deviation Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30v18DuDbIJh"
      },
      "source": [
        "Then, the evaluation of different standard deviation parameters begins. First, a list of lists, or a matrix, `values`, is initialized. The matrix will store objective function values, which are negative log-likelihood values, for datasets created for each deviation parameter. One row of the matrix will contain the values for one deviation parameter.\n",
        "\n",
        "A list `means_log` is initialized as zeros with a length of `n_devs`, and will store mean log-likelihood values for each deviation parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O4NBjTzua17"
      },
      "outputs": [],
      "source": [
        "values = [[] for i in range(n_devs)]\n",
        "means_log = [0.] * n_devs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaGuyhKuu2ym"
      },
      "source": [
        "####Negative Log-likelihood Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HksqJUfUjIjE"
      },
      "source": [
        "The actual evaluation is done in a for loop, which goes through each deviation parameter. The process is same than in previous notebooks, but as mentioned, the objective functions are created for each new dataset as synthetic likelihood is used. For each parameter, new dataset `newdata` is created, and its' size is the sum of `subset_sizes` variable.\n",
        "\n",
        "The `ecdf-estimator`'s `estimate_radii_values` is called to determine appropriate region values for the bin values. The values are selected based on the computed distances between data points of the first two subsets of the new data. The new data is divided to subsets based on `subset_sizes` variable. The returned region values are stored to `min_val` and `max_val` and the distances between data points to the matrix `distance_data`.\n",
        "\n",
        "Then, the following interval is split to 50 bins using `numpy`'s `linspace` function. Note that if the eCDF vectors would be wanted to plot, it should be done now by using the `bins` variable and creating auxiliary function. Then, the plotting could be done like demonstrated in previous notebooks. Also, the normality of eCDF vectors should be checked using and plotting the $\\chi^2$ test with `ecdf`'s functions. The plotting and normality checking is not done now, because it's not reasonable to be done `n_devs` times with each parameter which is tried.\n",
        "\n",
        "The objective function which is used to evaluate training data must be defined with smaller amount of bins. Otherwise there could be unwanted correlation between neighbouring bin values. The `estimator`'s `choose_bins` function is called to select the reasonable bin values from larger choice, and variable `bins` is overwritten with these values.\n",
        "\n",
        "The objective function `func` is assembled for each parameter by initializing a new instance of `estimator`'s `standard` or `bootstrap` class object with new dataset `newdata`. Then, in another for loop, each subset of the training data is evaluated with that function by calling the `ecdf`'s `evaluate` function. The command `data[sum(subset_sizes[0:j]):sum(subset_sizes[0:j+1])]` chooses the subsets of the training data which will be evaluated. If the training data is bigger than the sum of `subset_sizes` variable, all training data values are not used in the identification process. The returned negative log-likelihood values are stored into matrix `values`. After the inner loop, the log-likelihood values for current deviation parameter are computed, and the mean of them is computed and stored in list `means_log`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX0Bs6z1r03k"
      },
      "outputs": [],
      "source": [
        "for i in range(n_devs):\n",
        "  newdata = np.random.normal(mean_new, devs[i], np.sum(subset_sizes))\n",
        "  min_val, max_val, distance_data = ecdf.estimate_radii_values(newdata, subset_sizes, distance)\n",
        "\n",
        "  bins = np.linspace(min_val, max_val, 50)\n",
        "  bins = ecdf.choose_bins(distance_data, bins, n_bins)\n",
        "\n",
        "  if (ecdf_type == \"standard\"):\n",
        "    func = ecdf.standard(newdata, bins, distance, subset_sizes)\n",
        "  elif (ecdf_type == \"bootstrap\"):\n",
        "    func = ecdf.bootstrap(newdata, bins, distance, subset_sizes[0], subset_sizes[1])\n",
        "  else:\n",
        "    print(\"Invalid eCDF type.\")\n",
        "    break\n",
        "\n",
        "  for j in range(n_newsets):\n",
        "    values[i].append(ecdf.evaluate(func, data[sum(subset_sizes[0:j]):sum(subset_sizes[0:j+1])]))\n",
        "\n",
        "  means_log[i] = np.mean(values[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SVKqi3Z4rqa"
      },
      "source": [
        "The evaluated log-likelihood values for all datasets and their means are plotted using `matplotlib.pyplot`'s `plot`-function. Every dataset's value is plotted with red and the means for each deviation parameter with black. The smaller the mean value is to a certain parameter, the better the parameter fits the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmTq8M5Kv-Z5",
        "outputId": "5c4a66da-1b92-47bb-eee1-e909b9573371"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "\n",
        "ax.plot(devs, values, 'ro')\n",
        "ax.plot(devs, means_log, 'bo')\n",
        "plt.title(\"Evaluation of the Negative Log-likelihood Values\")\n",
        "plt.xlabel(\"Deviation Parameter\")\n",
        "plt.ylabel(\"Log-likelihood Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rap19f_J4wTP"
      },
      "source": [
        "####Normalization of Log-likelihood Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O70KmTW40du"
      },
      "source": [
        "The final step of the process is the normalization of the log-likelihood values. First, each value is multiplied with -0.5 and then exponentiated. This operation is now suitable because it turns the smallest negative log-likelihood values, which means the best fits to training data, to the biggest values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAuada-CwCOw"
      },
      "outputs": [],
      "source": [
        "values = [[np.exp(-0.5*values[i][j]) for j in range(n_newsets)] for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksuE_5-s43tg"
      },
      "source": [
        "Then, the sum of each column, which all include one log-likelihood value for each deviation parameter, is computed and stored in list `sums`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXQzYE58wEFN"
      },
      "outputs": [],
      "source": [
        "sums = np.sum(values, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCL6ZIKr4_pl"
      },
      "source": [
        "After that, each value in `values` is divided by the sum of its' column to get the likelihood values for each deviation parameter in the column to fit the training data. Basically, one dataset is selected for each parameter, and the likelihood value for each dataset is computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl_v0x__wGCu"
      },
      "outputs": [],
      "source": [
        "values = [[values[i][j] / sums[j] for j in range(n_newsets)] for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEVib_UV5IkW"
      },
      "source": [
        "The means of each row, which are the means of the datasets' likelihood values for each parameter, are computed and stored to list `means_nor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9JnSq9bwIgw"
      },
      "outputs": [],
      "source": [
        "means_nor = [np.mean(values[i]) for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us6ryhdE5MC1"
      },
      "source": [
        "Finally, the normalized likelihood values (red) and the means of them (blue) for each deviation parameter are plotted as before, and the goodness of each parameter can be estimated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_D0HiejwK4t",
        "outputId": "b8e79fb3-ccd8-43b3-c475-f306cd260771"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "\n",
        "ax.plot(devs, values, 'ro')\n",
        "ax.plot(devs, means_nor, 'bo')\n",
        "\n",
        "plt.title(\"Normalized Likelihood Values and Average Values Over All Evaluations\")\n",
        "plt.xlabel(\"Deviation Parameter\")\n",
        "plt.ylabel(\"Likelihood\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

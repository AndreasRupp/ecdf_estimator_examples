{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AndreasRupp/ecdf_estimator_examples/blob/bsc_thesis/tutorial/02-deviation-of-normal-distribution-bootstrap.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw581jzAtKrn"
      },
      "source": [
        "# Parameter Identification for Normal Distribution Data - Bootstrap Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGENMsdMuNrL"
      },
      "source": [
        "This notebook introduces a bootstrap parameter identification method for identifying deviation parameters in simple normal distribution data. The approach is based on the empirical cumulative distribution function (eCDF). First, normal distribution data is created using known deviation. Subsequently, new data is generated using various deviation parameters to identify the initial deviation parameter. The implementation of the method using Python and `ecdf-estimator` package is explained step-by-step within this notebook. The implementation of the method is very similar with the standard method demonstrated in the first notebook of the tutorial, and it is recommended to read first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPzHWHrOtYri"
      },
      "source": [
        "## Setup of the Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JUlvMfFv7LH"
      },
      "source": [
        "###Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otTIJExbD4hX"
      },
      "source": [
        "First, `ecdf-estimator` is installed and imported as `ecdf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5jxKQKLDcTz",
        "outputId": "cab32b5a-69ec-4f7e-e59c-66104ad62d9e"
      },
      "outputs": [],
      "source": [
        "pip install ecdf-estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0046SdnzDb5u"
      },
      "source": [
        "After the installation, the estimator is imported so that its' functions can actually be used. It's abbreviated as `ecdf` so it's easier and faster to write in the code and makes it easier to read. Now, the `ecdf`'s functions can be called like `ecdf.function_name()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTZ6k4oFd-vI"
      },
      "outputs": [],
      "source": [
        "import ecdf_estimator as ecdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7gXPHBBehWs"
      },
      "source": [
        "Next, the other two necessary modules, `numpy` and `matplotlib.pyplot`, are imported. The modules are used to do array operations efficiently and to visualize the data and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twIKoX9wDVai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh62ODUhn3V-"
      },
      "source": [
        "### Creation of Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhPZ21EtiTbI"
      },
      "source": [
        "Next, normally distributed training data must be created. First, the variables which are used to create the data are defined:\n",
        "\n",
        "* `mean`: The mean value of training data.<br>\n",
        "* `dev`: The training data's standard deviation, which is tried to identidy.<br>\n",
        "* `size`: The number of created data points.<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMTMtAgajioV"
      },
      "outputs": [],
      "source": [
        "mean = 0\n",
        "dev = 5\n",
        "size = 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye7XlH1gjzEm"
      },
      "source": [
        "After the parameters are defined, the normally distibuted training data can be created using them and `numpy`'s `random.normal` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU8WM26tj6B8"
      },
      "outputs": [],
      "source": [
        "data = np.random.normal(mean, dev, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejcsNrVAsNeW"
      },
      "source": [
        "If desired, noise can be added to the training data by setting the variable `noise_dev` as the deviation of the noise distribution. By default, `noise_dev` is set as `None` and noise is not added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGkuqyLlsgu0"
      },
      "outputs": [],
      "source": [
        "noise_dev = None\n",
        "\n",
        "if noise_dev is not None:\n",
        "  data += np.random.normal(mean, noise_dev, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgeJzS7VoJGQ"
      },
      "source": [
        "## Parameter Identification Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boO1_kHpxBVr"
      },
      "source": [
        "After the creation of training data, the actual parameter identification begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpzsbQl8oR90"
      },
      "source": [
        "### Definition of Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ejERZbwl98"
      },
      "source": [
        "The variables which will be used in the process are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBGM5Qcow_iS"
      },
      "source": [
        "* `subset_sizes`: With bootstrapping, the first two items of this list define the sizes of subsets, which are selected randomly from the training data later in the process when creating eCDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIC9LCF6yVIU"
      },
      "outputs": [],
      "source": [
        "subset_sizes = [150] * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4ao1CKyoOz"
      },
      "source": [
        "* `mean_new`: The mean value of new datasets to be created. In practice, this is now the same as the mean of the training data, but this variable is defined for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZZ29OSE08H9"
      },
      "outputs": [],
      "source": [
        "mean_new = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU-4poFa1Bpp"
      },
      "source": [
        "* `n_bins`: The maximum number of bins which will be used for the eCDF-vectors. The exact number will be defined by the `ecdf`'s own functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8B2ouP01pWD"
      },
      "outputs": [],
      "source": [
        "n_bins = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8I1Ctr41lj5"
      },
      "source": [
        "- `min_dev`: The start parameter to be estimated.<br>\n",
        "- `max_dev`: The end parameter to be estimated.<br>\n",
        "\n",
        "`min_dev` and `max_dev` define the interval between which all integer values are tested to be the standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et37Mfsh3Pss"
      },
      "outputs": [],
      "source": [
        "min_dev = 2\n",
        "max_dev = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nki3sB7A9gF6"
      },
      "source": [
        "Next, a function named `distance`, which has a major part in the creation of eCDF-vectors in the whole identification process, must be defined. The function computes the absolute value of the subtraction of the data points, which is the so called Euclidian distance between the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWoob9HY9ppb"
      },
      "outputs": [],
      "source": [
        "def distance(data_a, data_b):\n",
        "  return np.abs(data_a-data_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttifj5PyojZK"
      },
      "source": [
        "###Generation of Objective Functions and eCDF-vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUJshfogjey0"
      },
      "source": [
        "Next, the eCDF-vectors have to be created and plotted. This is done by initializing a new instance of `ecdf`'s `bootstrap` class object function. Because the vectors are wanted to plot using a large number of bins and with smaller amount of bins, they must be created twice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf9Uy0gNVPns"
      },
      "source": [
        "The `ecdf-estimator`'s `estimate_radii_values` is called to determine appropriate region values for the bin values. The values are selected based on the computed distances between data points of the first two subsets of the training data. The returned region values are stored to `min_val` and `max_val` and the distances between data points to the matrix `distance_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYoZd4BkVhMC"
      },
      "outputs": [],
      "source": [
        "min_val, max_val, distance_data = ecdf.estimate_radii_values(data, subset_sizes, distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1BfQIm3atWq"
      },
      "source": [
        "Then, the following interval is split into 50 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrtDVRlccd31"
      },
      "outputs": [],
      "source": [
        "bins = np.linspace(min_val, max_val, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-4VHEa9ckEx"
      },
      "source": [
        "The objective function is assembled by initializing a new instance of `estimator`'s `bootstrap` class object. `data`, `bins`, `distance`-function, `subset_sizes[0]` and `subset_sizes[1]` are given as input arguments. In this part, there is a notable difference compared to standard method. Now, the whole dataset is not divided to different subsets. Instead, distances between all single values of the data points are calculated and a distance matrix is created from the distances. Then, `subset_sizes[0]` and `subset_sizes[1]` distances are selected randomly from the distance matrix (so that the same values can't be selected twice) to form two subsets. Distances between these subsets are then computed with `distance` function and a eCDF vector of these distances is created. This selection of subsets and creation of eCDF vectors is done 1000 times. All values computed are stored to `aux_func` to specify the first objective function with all 50 bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VllOYVSDcrpw"
      },
      "outputs": [],
      "source": [
        "aux_func = ecdf.bootstrap(data, bins, distance, subset_sizes[0], subset_sizes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFfeGwQ5d8WM"
      },
      "source": [
        "Then, the other function with smaller amount of bins must be defined. Otherwise there could be unwanted correlation between neighbouring bin values. The `estimator`'s `choose_bins`-function is called to select the reasonable bin values from larger choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yIoyMgrkKFu"
      },
      "outputs": [],
      "source": [
        "bins = ecdf.choose_bins(distance_data, bins, n_bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9imsOs4lFs5"
      },
      "source": [
        "Now, the second objective function `func` can be defined. This time, only the small amount of bins is used to create the eCDF-vectors and compute the statistics for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrP5JID6lzjV"
      },
      "outputs": [],
      "source": [
        "func = ecdf.bootstrap(data, bins, distance, subset_sizes[0], subset_sizes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AofbHkoXq494"
      },
      "source": [
        "After the two objective functions have been created, they can be plotted. First, the figure is defined using `matplotlib.pyplot`'s `subplots`-function. Then, the plots are made using the `estimator`'s functions. The vectors with 50 bins are plotted as purple, the ones with selected bins only as light blue, and their mean values as black."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "8jTnoV20reMd",
        "outputId": "c5998f21-a5ea-4dbe-92c1-29f4ab4f3794"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ecdf.plot_ecdf_vectors(aux_func, ax, 'm.')\n",
        "ecdf.plot_ecdf_vectors(func, ax, 'c.')\n",
        "ecdf.plot_mean_vector(func, ax, 'k.')\n",
        "plt.title(\"Distribution of the eCDF-vectors\")\n",
        "plt.xlabel(\"Bin Values\")\n",
        "plt.ylabel(\"Cumulative Probability\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1RrXx84pJyA"
      },
      "source": [
        "###$\\chi^2$ test: Checking normality of eCDF vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2t-Ed0By7m1"
      },
      "source": [
        "The eCDF-vectors should be Gaussian with big enough sample size of training data. This is ensured using the $\\chi^2$ test which is done by calling the `estimator`'s `plot_chi2_test`-function. The function computes the negative log-likelihood values for each eCDF vector to examine how well each vector fits the objective function `func`'s model. The log-likelihood values are then normalized and a histogram of them is created and plotted. After that, the probability density function of the chi-square distribution with an appropriate degree of freedom (which is the number of the eCDF vectors) is plotted on top of the histogram. If the histogram fits the density function well, the normality of eCDF vectors is confirmed and the parameter estimation process should be valid and reliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "jFVvXOCnzUZF",
        "outputId": "9b0715d2-d90a-4973-c851-526719652778"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ecdf.plot_chi2_test(func, ax)\n",
        "plt.title(\"Gaussianity Test by Chi-squared -criterion\")\n",
        "plt.xlabel(\"Normalized Log-likelihood\")\n",
        "plt.ylabel(\"Probability Density\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vIHd7vHpPjr"
      },
      "source": [
        "###Evaluation of Deviation Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjGYiP_h3423"
      },
      "source": [
        "After ensuring the reliability of the identification process, the evaluation of different standard deviation parameters begins. First, a list `devs` containing all the deviation parameters which will be evaluated, is created. The number of parameters is stored to `n_devs`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWPf2wjF4PJ1"
      },
      "outputs": [],
      "source": [
        "devs = list(range(min_dev, max_dev + 1))\n",
        "n_devs = len(devs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wYTu9c3AjVI"
      },
      "source": [
        "Next, a list of lists, or a matrix, `values`, is initialized. The matrix will store objective function values, which are negative log-likelihood values, for datasets created for each deviation parameter. One row of the matrix will contain the values for one deviation parameter.\n",
        "\n",
        "List `means_log` is initialized as zeros with a length of `n_devs`, and will store mean log-likelihood values for each deviation parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKTOMJoxAeQ-"
      },
      "outputs": [],
      "source": [
        "values = [[] for i in range(n_devs)]\n",
        "means_log = [0.] * n_devs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1tVxBQbslWH"
      },
      "source": [
        "A variable `n_subsets` defines how many new datasets are created for each deviation parameter and evaluated with the objective function. By default, 5 subsets are evaluated for each parameter. It is good to note that with bootstrapping also smaller number of subsets may be enough to get decent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ6i93cgsj3_"
      },
      "outputs": [],
      "source": [
        "n_subsets = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1wQGJnKqGux"
      },
      "source": [
        "####Negative Log-likelihood Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpNOlZkkC07J"
      },
      "source": [
        "The creation and evaluation of new datasets starts and the negative log-likelihood values will be computed. The loop iterates `n_devs` times, and in each iteration loop, a new dataset `newdata` with the current deviation parameter `devs[i]` is created using numpy's `random.normal`-function, just like with the creation of training data. The size of a new dataset is the same as the dataset size in training data, `subset_sizes[0]`.\n",
        "\n",
        "Then, the `estimator`'s `evaluate`-function is called to evaluate the new datasets with the objective function `func`. After a new dataset `newdata` is created, another new dataset is formed from it by changing its' order. An eCDF vector of the distances between these datasets is created. Then, the negative log-likelihood value of the new eCDF vector is calculated to examine how it fits the objective function created with training data. This value is the one which the function returns, and it is stored into list `values` by appending it to the place of the current deviation parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckykUEOEAf_b"
      },
      "outputs": [],
      "source": [
        "for i in range(n_devs):\n",
        "  for j in range(n_subsets):\n",
        "    newdata = np.random.normal(mean_new, devs[i], subset_sizes[0])\n",
        "    values[i].append(ecdf.evaluate(func, newdata))\n",
        "  means_log[i] = np.mean(values[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UBGxelcgUND"
      },
      "source": [
        "The evaluated log-likelihood values for all datasets and their means are plotted using `matplotlib.pyplot`'s `plot`-function. Every dataset's value is plotted with red and the means for each deviation parameter with black. The smaller the mean value is to a certain parameter, the better the parameter fits the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "gapZB1k3QU6D",
        "outputId": "c110333d-b8f2-4c79-cc84-1663923d42ff"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(devs, values, 'ro')\n",
        "ax.plot(devs, means_log, 'bo')\n",
        "plt.title(\"Evaluation of the Negative Log-likelihood Values\")\n",
        "plt.xlabel(\"Deviation Parameter\")\n",
        "plt.ylabel(\"Log-likelihood Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHIr3P4PqSEN"
      },
      "source": [
        "####Normalization of Log-likelihood Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoP0UsxA2JeK"
      },
      "source": [
        "The final step of the process is the normalization of the log-likelihood values. First, each value is multiplied with -0.5 and then exponentiated. This operation is now suitable because it turns the smallest negative log-likelihood values, which means the best fits to training data, to the biggest values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75K_vJ662KZv"
      },
      "outputs": [],
      "source": [
        "values = [[np.exp(-0.5*values[i][j]) for j in range(n_subsets)] for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7sQMmT6vIx"
      },
      "source": [
        "Then, the sum of each column, which all include one log-likelihood value for each deviation parameter, is computed and stored in list `sums`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L35qcN_h7lVp"
      },
      "outputs": [],
      "source": [
        "sums = np.sum(values, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ponz1fIj7wqn"
      },
      "source": [
        "After that, each value in `values` is divided by the sum of its' column to get the likelihood values for each deviation parameter in the column to fit the training data. Basically, one dataset is selected for each parameter, and the likelihood value for each dataset is computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SE4X3L_-A1t"
      },
      "outputs": [],
      "source": [
        "values = [[values[i][j] / sums[j] for j in range(n_subsets)] for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqUgilkE-Eze"
      },
      "source": [
        "The means of each row, which are the means of the datasets' likelihood values for each parameter, are computed and stored to list `means_nor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XojcJpcQsc43"
      },
      "outputs": [],
      "source": [
        "means_nor = [np.mean(values[i]) for i in range(n_devs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UthgHuTsASFd"
      },
      "source": [
        "Finally, the normalized likelihood values for each deviation parameter are plotted as before, and the goodness of each parameter can be estimated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "keWwvRCAroni",
        "outputId": "19cca431-9269-4075-b8bf-83a7b66f96af"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(devs, values, 'ro')\n",
        "ax.plot(devs, means_nor, 'bo')\n",
        "#plt.title(\"Normalized Likelihood Values and Average Values Over All Evaluations\")\n",
        "plt.xlabel(\"Deviation Parameter\")\n",
        "plt.ylabel(\"Likelihood\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
